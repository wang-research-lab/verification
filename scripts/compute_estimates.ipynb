{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d17c865",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6595939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from verification.utils import load_jsonl\n",
    "aime2025 = load_jsonl(\"../data/aime2025.jsonl\")\n",
    "aime2025_qwen32b = [x for x in aime2025 if x[\"llm_responses\"][0][\"model\"] == \"deepseek-ai/DeepSeek-R1-Distill-Qwen-32B\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "578e8bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verifier_flops(d_model, d_inter, n_layers, vocab_size, T_in, T_out):\n",
    "    \"\"\"\n",
    "    FLOPs estimate for a decoder-only Transformer with KV cache.\n",
    "    - d_model: hidden size (d)\n",
    "    - d_inter: MLP intermediate size (m)\n",
    "    - n_layers: number of transformer layers (L)\n",
    "    - vocab_size: vocab size (V); set to 1 for discriminative\n",
    "    - T_in: input/prefill tokens\n",
    "    - T_out: output tokens; for discriminative, usually 1\n",
    "    \"\"\"\n",
    "    d, m, L, V = d_model, d_inter, n_layers, vocab_size\n",
    "\n",
    "    # per-layer, per-token constants (QKV+O + MLP)\n",
    "    const_per_layer_token = 8*d*d + 4*d*m\n",
    "\n",
    "    # attention coeff per layer (scales with context length)\n",
    "    attn_coeff_per_layer = 4*d\n",
    "\n",
    "    # Prefill\n",
    "    prefill_ctx_sum = T_in * (T_in + 1) // 2\n",
    "    flops_prefill = L * (\n",
    "        const_per_layer_token * T_in +\n",
    "        attn_coeff_per_layer  * prefill_ctx_sum\n",
    "    )\n",
    "\n",
    "    # Decode\n",
    "    decode_ctx_sum = T_in * T_out + (T_out - 1) * T_out // 2 if T_out > 0 else 0\n",
    "    flops_decode_layers = L * (\n",
    "        const_per_layer_token * T_out +\n",
    "        attn_coeff_per_layer  * decode_ctx_sum\n",
    "    )\n",
    "\n",
    "    # LM head (de-embedding)\n",
    "    flops_lm_head = 2 * d * V * T_out\n",
    "\n",
    "    return flops_prefill + flops_decode_layers + flops_lm_head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1f8264",
   "metadata": {},
   "source": [
    "solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ff9ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16387 > 16384). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE_SOLVER_INPUT_LENGTH=133\n",
      "AVERAGE_SOLVER_OUTPUT_LENGTH=10602\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_lengths = []\n",
    "output_lengths = []\n",
    "\n",
    "for item in aime2025_qwen32b:\n",
    "    propmt = item[\"prompt\"]\n",
    "    prompt_length = len(tokenizer.encode(propmt))\n",
    "    input_lengths.append(prompt_length)\n",
    "\n",
    "    for response_item in item[\"llm_responses\"]:\n",
    "        response = response_item[\"response\"]\n",
    "        output_length = len(tokenizer.encode(response))\n",
    "        output_lengths.append(output_length)\n",
    "\n",
    "AVERAGE_SOLVER_INPUT_LENGTH = int(np.mean(input_lengths))\n",
    "AVERAGE_SOLVER_OUTPUT_LENGTH = int(np.mean(output_lengths))\n",
    "print(f\"{AVERAGE_SOLVER_INPUT_LENGTH=}\")\n",
    "print(f\"{AVERAGE_SOLVER_OUTPUT_LENGTH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4373e3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625131826708480"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AVERAGE_SOLVER_FLOPS_PER_SAMPLE = verifier_flops(5120, 27648, 64, 152064, AVERAGE_SOLVER_INPUT_LENGTH, AVERAGE_SOLVER_OUTPUT_LENGTH)\n",
    "AVERAGE_SOLVER_FLOPS_PER_SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b998335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts/calc_latency.py\n",
    "AVERAGE_SOLVER_LATENCY_PER_SAMPLE_N_1 = 273.074387550354\n",
    "AVERAGE_SOLVER_LATENCY_PER_SAMPLE_N_2 = 276.6106116771698\n",
    "AVERAGE_SOLVER_LATENCY_PER_SAMPLE_N_4 = 288.38619780540466\n",
    "AVERAGE_SOLVER_LATENCY_PER_SAMPLE_N_8 = 448.35090923309326\n",
    "AVERAGE_SOLVER_LATENCY_PER_SAMPLE_N_16 = 782.906281709671\n",
    "AVERAGE_SOLVER_LATENCY_PER_SAMPLE_N_32 = 1439.957554101944\n",
    "AVERAGE_SOLVER_LATENCY_PER_SAMPLE_N_64 = 2815.468241930008\n",
    "AVERAGE_SOLVER_LATENCY_PER_SAMPLE_N_128 = 5514.078681468964"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2dc402",
   "metadata": {},
   "source": [
    "heimdall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "060fd7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE_HEIMDALL_INPUT_LENGTH=5525\n",
      "AVERAGE_HEIMDALL_OUTPUT_LENGTH=10500\n"
     ]
    }
   ],
   "source": [
    "input_lengths = []\n",
    "\n",
    "heimdall_prompt = \"\"\"\n",
    "Here is a math problem and a solution of it. Think step by step and verify if the final answer in the\n",
    "solution is correct. The last line of your response should be of the form Answer: $Answer (without\n",
    "quotes) where $Answer is 1 if the final answer in the solution is correct and 0 if incorrect.\n",
    "\n",
    "**Problem**\n",
    "${problem}\n",
    "\n",
    "**Solution**\n",
    "${solution}\n",
    "\"\"\".strip()\n",
    "\n",
    "for item in aime2025_qwen32b:\n",
    "    problem = item[\"prompt\"]\n",
    "    for response_item in item[\"llm_responses\"]:\n",
    "        solution = response_item[\"response\"]\n",
    "        solution = solution.split(\"</think>\")[-1]\n",
    "\n",
    "    prompt = heimdall_prompt.format(problem=problem, solution=solution)\n",
    "    input_length = len(tokenizer.encode(prompt))\n",
    "    input_lengths.append(input_length)\n",
    "\n",
    "AVERAGE_HEIMDALL_INPUT_LENGTH = int(np.mean(input_lengths))\n",
    "print(f\"{AVERAGE_HEIMDALL_INPUT_LENGTH=}\")\n",
    "\n",
    "AVERAGE_HEIMDALL_OUTPUT_LENGTH = 10500 # extrapolated from paper's results on aime 2024\n",
    "print(f\"{AVERAGE_HEIMDALL_OUTPUT_LENGTH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4167974c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "980453982208000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AVERAGE_HEIMDALL_FLOPS_PER_SAMPLE = verifier_flops(5120, 27648, 64, 152064, AVERAGE_HEIMDALL_INPUT_LENGTH, AVERAGE_HEIMDALL_OUTPUT_LENGTH)\n",
    "AVERAGE_HEIMDALL_FLOPS_PER_SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8227d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts/calc_latency.py\n",
    "AVERAGE_HEIMDALL_LATENCY_PER_SAMPLE_N_1 = 276.01961755752563\n",
    "AVERAGE_HEIMDALL_LATENCY_PER_SAMPLE_N_2 = 279.4113857746124\n",
    "AVERAGE_HEIMDALL_LATENCY_PER_SAMPLE_N_4 = 328.29437160491943\n",
    "AVERAGE_HEIMDALL_LATENCY_PER_SAMPLE_N_8 = 496.405907869339\n",
    "AVERAGE_HEIMDALL_LATENCY_PER_SAMPLE_N_16 = 912.8740525245667\n",
    "AVERAGE_HEIMDALL_LATENCY_PER_SAMPLE_N_32 = 1711.8473834991455\n",
    "AVERAGE_HEIMDALL_LATENCY_PER_SAMPLE_N_64 = 3334.3926618099213\n",
    "AVERAGE_HEIMDALL_LATENCY_PER_SAMPLE_N_128 = 6580.33390879631"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3370a4",
   "metadata": {},
   "source": [
    "our dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a74b04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (16546 > 16384). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVERAGE_DV_INPUT_LENGTH=5082\n",
      "AVERAGE_DV_OUTPUT_LENGTH=1\n"
     ]
    }
   ],
   "source": [
    "input_lengths = []\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "\n",
    "for item in aime2025_qwen32b:\n",
    "    prompt = item[\"prompt\"]\n",
    "    for response_item in item[\"llm_responses\"]:\n",
    "        response = response_item[\"response\"]\n",
    "\n",
    "        inputs = tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "                {\"role\": \"assistant\", \"content\": response.split(\"</think>\")[-1]}\n",
    "            ],\n",
    "            tokenize=True, \n",
    "            continue_final_message=False,\n",
    "        )\n",
    "\n",
    "        input_lengths.append(len(inputs))\n",
    "\n",
    "AVERAGE_DV_INPUT_LENGTH = int(np.mean(input_lengths))\n",
    "print(f\"{AVERAGE_DV_INPUT_LENGTH=}\")\n",
    "\n",
    "AVERAGE_DV_OUTPUT_LENGTH = 1 # single forward pass\n",
    "print(f\"{AVERAGE_DV_OUTPUT_LENGTH=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6ef30d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12744068803584"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AVERAGE_DV_FLOPS_PER_SAMPLE = verifier_flops(1536, 8960, 28, 1, AVERAGE_DV_INPUT_LENGTH, AVERAGE_DV_OUTPUT_LENGTH) # vocab size is 1 since scalar head\n",
    "AVERAGE_DV_FLOPS_PER_SAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf482efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "AVERAGE_DV_LATENCY_PER_SAMPLE = 0.05195287466049194 # from scripts/calc_latency.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "verification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
